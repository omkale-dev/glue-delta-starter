{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.functions import col\n",
    "from delta.tables import DeltaTable\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "import json\n",
    "import importlib\n",
    "from pyspark.sql.functions import col, coalesce, lit\n",
    "import helper_functions\n",
    "importlib.reload(helper_functions)\n",
    "\n",
    "LOCAL = True\n",
    "job_name = \"the_glue_job_one\"\n",
    "\n",
    "source_path = \"s3://<bucket>/d_customer/d_customer_two.csv\"\n",
    "mapping_path = f\"./jupyter_workspace/{job_name}/the_resources/mapping.csv\"\n",
    "delta_bucket = \"s3://<delta-bucket>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def glue_init():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    glueContext = GlueContext(sc)\n",
    "    # spark = glueContext.spark_session\n",
    "    job = Job(glueContext)\n",
    "    # spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "    if not LOCAL:\n",
    "        global source_path, mapping_path\n",
    "        args = getResolvedOptions(sys.argv, [\"source_path\"])\n",
    "        source_path = args[\"source_path\"]\n",
    "        print(source_path)\n",
    "        mapping_path = \"./mapping.csv\"\n",
    "\n",
    "    return spark, glueContext, job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def extract(glueContext, source_file):\n",
    "    source_df = helper_functions.get_source_df(glueContext, source_file)\n",
    "    return source_df\n",
    "\n",
    "\n",
    "def first_load(full_load_df, delta_bucket):\n",
    "    full_load_df.write.format(\"delta\").mode(\n",
    "        \"overwrite\").save(delta_bucket+\"/delta/customer/\")\n",
    "\n",
    "\n",
    "def delta_scd(spark, cdc_df, delta_bucket):\n",
    "    delta_df = DeltaTable.forPath(spark, delta_bucket + \"/delta/customer/\")\n",
    "    # UPSERT process if matches on the condition the update else insert\n",
    "    # if there is no keyword then create a data set with Insert, Update and Delete flag and do it separately.\n",
    "    # for delete it has to run in loop with delete condition, this script do not handle deletes.\n",
    "    delta_df.alias(\"prev_df\").merge(\n",
    "        source=cdc_df.alias(\"append_df\"), \\\n",
    "        # matching on primarykey\n",
    "        condition=expr(\"prev_df.customer_id = append_df.customer_id\"))\\\n",
    "        .whenMatchedUpdate(\n",
    "        set={\n",
    "            \"prev_df.customer_name\": col(\"append_df.customer_name\"),\n",
    "            \"prev_df.state\": col(\"append_df.state\"),\n",
    "            \"prev_df.is_active\": col(\"append_df.is_active\")\n",
    "        })\\\n",
    "        .whenNotMatchedInsert(\n",
    "        values={\n",
    "            \"prev_df.customer_id\": col(\"append_df.customer_id\"),\n",
    "            \"prev_df.customer_name\": col(\"append_df.customer_name\"),\n",
    "            \"prev_df.state\": col(\"append_df.state\"),\n",
    "            \"prev_df.is_active\": col(\"append_df.is_active\")\n",
    "        }).execute()\n",
    "    print(\"SCD load completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    spark, glueContext, job = glue_init()\n",
    "    mapping = helper_functions.get_csv_mapping(mapping_path)\n",
    "    source_df = extract(glueContext, source_path)\n",
    "    # first_load(source_df,delta_bucket)\n",
    "    delta_scd(spark, source_df, delta_bucket)\\"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
